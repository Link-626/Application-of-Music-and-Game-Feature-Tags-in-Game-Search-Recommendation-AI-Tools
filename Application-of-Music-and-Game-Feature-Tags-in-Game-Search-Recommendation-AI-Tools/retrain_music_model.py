#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡æ–°è®­ç»ƒéŸ³ä¹åˆ†ç±»æ¨¡å‹
"""

import os
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

class MusicStyleDataset(Dataset):
    """éŸ³ä¹é£æ ¼æ•°æ®é›†"""
    def __init__(self, features, labels):
        self.features = torch.FloatTensor(features)
        self.labels = torch.LongTensor(labels)
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

class MusicStyleNet(nn.Module):
    """éŸ³ä¹é£æ ¼åˆ†ç±»ç¥ç»ç½‘ç»œ"""
    def __init__(self, input_size, num_classes, hidden_sizes=[512, 256, 128, 64]):
        super(MusicStyleNet, self).__init__()
        
        layers = []
        prev_size = input_size
        
        # æ„å»ºéšè—å±‚
        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.BatchNorm1d(hidden_size),
                nn.ReLU(),
                nn.Dropout(0.4)
            ])
            prev_size = hidden_size
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_size, num_classes))
        
        self.network = nn.Sequential(*layers)
        
    def forward(self, x):
        return self.network(x)

def generate_realistic_training_data():
    """ç”Ÿæˆæ›´çœŸå®çš„è®­ç»ƒæ•°æ®"""
    print("ç”Ÿæˆè®­ç»ƒæ•°æ®...")
    
    # éŸ³ä¹é£æ ¼æ ‡ç­¾
    label_encoder = {
        'electronic': 0, 'rock': 1, 'classical': 2, 'ambient': 3,
        'pop': 4, 'jazz': 5, 'hip_hop': 6, 'folk': 7,
        'chinese_traditional': 8, 'pixel_game': 9
    }
    
    # ä¸ºæ¯ç§é£æ ¼å®šä¹‰æ›´çœŸå®çš„ç‰¹å¾åˆ†å¸ƒï¼ˆåŸºäºå®é™…éŸ³é¢‘ç‰¹å¾è°ƒæ•´ï¼‰
    style_distributions = {
        'electronic': {
            'tempo': (120, 180), 'centroid': (1500, 3500), 'bandwidth': (2000, 5000),
            'rolloff': (4000, 10000), 'zcr': (0.05, 0.15), 'rms': (0.4, 0.8),
            'mfcc_range': (-15, 80), 'chroma_range': (0.01, 1.0),
            'centroid_std': (200, 600), 'bandwidth_std': (400, 1000),
            'rolloff_std': (800, 2000), 'zcr_std': (0.02, 0.08)
        },
        'rock': {
            'tempo': (100, 160), 'centroid': (1800, 3500), 'bandwidth': (2500, 4500),
            'rolloff': (5000, 9000), 'zcr': (0.1, 0.25), 'rms': (0.3, 0.7),
            'mfcc_range': (-12, 70), 'chroma_range': (0.1, 1.0),
            'centroid_std': (300, 700), 'bandwidth_std': (500, 1100),
            'rolloff_std': (800, 1800), 'zcr_std': (0.03, 0.09)
        },
        'classical': {
            'tempo': (60, 120), 'centroid': (1500, 2800), 'bandwidth': (2000, 3500),
            'rolloff': (3500, 7000), 'zcr': (0.03, 0.12), 'rms': (0.2, 0.5),
            'mfcc_range': (-8, 60), 'chroma_range': (0.2, 0.9),
            'centroid_std': (200, 500), 'bandwidth_std': (300, 700),
            'rolloff_std': (600, 1400), 'zcr_std': (0.01, 0.06)
        },
        'ambient': {
            'tempo': (60, 100), 'centroid': (1000, 2500), 'bandwidth': (1500, 3000),
            'rolloff': (3000, 6000), 'zcr': (0.02, 0.08), 'rms': (0.1, 0.3),
            'mfcc_range': (-5, 50), 'chroma_range': (0.01, 0.8),
            'centroid_std': (150, 400), 'bandwidth_std': (200, 500),
            'rolloff_std': (400, 1000), 'zcr_std': (0.01, 0.04)
        },
        'pop': {
            'tempo': (100, 140), 'centroid': (1700, 3200), 'bandwidth': (2200, 4000),
            'rolloff': (4500, 8000), 'zcr': (0.08, 0.18), 'rms': (0.25, 0.6),
            'mfcc_range': (-10, 65), 'chroma_range': (0.1, 1.0),
            'centroid_std': (250, 600), 'bandwidth_std': (350, 800),
            'rolloff_std': (600, 1500), 'zcr_std': (0.02, 0.07)
        },
        'jazz': {
            'tempo': (80, 160), 'centroid': (1600, 3000), 'bandwidth': (2000, 3500),
            'rolloff': (3500, 7000), 'zcr': (0.05, 0.15), 'rms': (0.2, 0.5),
            'mfcc_range': (-9, 55), 'chroma_range': (0.2, 0.8),
            'centroid_std': (200, 550), 'bandwidth_std': (300, 650),
            'rolloff_std': (500, 1300), 'zcr_std': (0.02, 0.06)
        },
        'hip_hop': {
            'tempo': (70, 140), 'centroid': (1800, 3200), 'bandwidth': (2200, 4000),
            'rolloff': (4500, 8000), 'zcr': (0.06, 0.16), 'rms': (0.3, 0.7),
            'mfcc_range': (-11, 60), 'chroma_range': (0.1, 0.9),
            'centroid_std': (250, 650), 'bandwidth_std': (350, 850),
            'rolloff_std': (700, 1600), 'zcr_std': (0.02, 0.08)
        },
        'folk': {
            'tempo': (80, 120), 'centroid': (1400, 2800), 'bandwidth': (1800, 3200),
            'rolloff': (3000, 6000), 'zcr': (0.04, 0.12), 'rms': (0.15, 0.4),
            'mfcc_range': (-7, 45), 'chroma_range': (0.2, 0.7),
            'centroid_std': (180, 450), 'bandwidth_std': (220, 550),
            'rolloff_std': (450, 1200), 'zcr_std': (0.01, 0.05)
        },
        'chinese_traditional': {
            'tempo': (60, 100), 'centroid': (1300, 2500), 'bandwidth': (1500, 2800),
            'rolloff': (2800, 5500), 'zcr': (0.03, 0.1), 'rms': (0.1, 0.3),
            'mfcc_range': (-6, 40), 'chroma_range': (0.15, 0.8),
            'centroid_std': (150, 400), 'bandwidth_std': (180, 500),
            'rolloff_std': (400, 1000), 'zcr_std': (0.01, 0.04)
        },
        'pixel_game': {
            'tempo': (100, 160), 'centroid': (1600, 3000), 'bandwidth': (2000, 3500),
            'rolloff': (3500, 7000), 'zcr': (0.05, 0.15), 'rms': (0.2, 0.5),
            'mfcc_range': (-8, 50), 'chroma_range': (0.1, 0.9),
            'centroid_std': (200, 500), 'bandwidth_std': (250, 600),
            'rolloff_std': (500, 1300), 'zcr_std': (0.02, 0.06)
        }
    }
    
    X, y = [], []
    
    for style, dist in style_distributions.items():
        print(f"ç”Ÿæˆ {style} é£æ ¼æ•°æ®...")
        for _ in range(500):  # æ¯ç§é£æ ¼500ä¸ªæ ·æœ¬
            # ç”ŸæˆåŸºç¡€ç‰¹å¾
            tempo = np.random.uniform(dist['tempo'][0], dist['tempo'][1])
            centroid = np.random.uniform(dist['centroid'][0], dist['centroid'][1])
            bandwidth = np.random.uniform(dist['bandwidth'][0], dist['bandwidth'][1])
            rolloff = np.random.uniform(dist['rolloff'][0], dist['rolloff'][1])
            zcr = np.random.uniform(dist['zcr'][0], dist['zcr'][1])
            rms = np.random.uniform(dist['rms'][0], dist['rms'][1])
            
            # ç”ŸæˆMFCCç‰¹å¾ (13ç»´)
            mfcc_mean = np.random.uniform(dist['mfcc_range'][0], dist['mfcc_range'][1], 13)
            mfcc_std = np.random.uniform(1, 5, 13)
            
            # ç”Ÿæˆè‰²åº¦ç‰¹å¾ (12ç»´)
            chroma_mean = np.random.uniform(dist['chroma_range'][0], dist['chroma_range'][1], 12)
            chroma_std = np.random.uniform(0.1, 0.3, 12)
            
            # å…¶ä»–ç»Ÿè®¡ç‰¹å¾
            centroid_std = np.random.uniform(dist['centroid_std'][0], dist['centroid_std'][1])
            bandwidth_std = np.random.uniform(dist['bandwidth_std'][0], dist['bandwidth_std'][1])
            rolloff_std = np.random.uniform(dist['rolloff_std'][0], dist['rolloff_std'][1])
            zcr_std = np.random.uniform(dist['zcr_std'][0], dist['zcr_std'][1])
            
            # æ„å»ºç‰¹å¾å‘é‡ (64ç»´)
            feature_vector = [
                tempo, centroid, bandwidth, rolloff, zcr, rms,
                np.mean(mfcc_mean), np.std(mfcc_mean),
                np.mean(chroma_mean), np.std(chroma_mean)
            ]
            feature_vector.extend(mfcc_mean)
            feature_vector.extend(mfcc_std)
            feature_vector.extend(chroma_mean)
            feature_vector.extend(chroma_std)
            feature_vector.extend([centroid_std, bandwidth_std, rolloff_std, zcr_std])
            
            X.append(feature_vector)
            y.append(label_encoder[style])
    
    return np.array(X), np.array(y), label_encoder

def plot_training_curves(train_losses, val_losses, val_accuracies):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    plt.figure(figsize=(15, 5))
    
    # è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±
    plt.subplot(1, 3, 1)
    plt.plot(train_losses, label='Training Loss', color='blue')
    plt.plot(val_losses, label='Validation Loss', color='red')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    # éªŒè¯å‡†ç¡®ç‡
    plt.subplot(1, 3, 2)
    plt.plot(val_accuracies, label='Validation Accuracy', color='green')
    plt.title('Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    
    # è®­ç»ƒè¿›åº¦
    plt.subplot(1, 3, 3)
    epochs = range(1, len(train_losses) + 1)
    plt.plot(epochs, train_losses, 'b-', label='Training Loss')
    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')
    plt.plot(epochs, val_accuracies, 'g-', label='Validation Accuracy')
    plt.title('Training Progress Overview')
    plt.xlabel('Epoch')
    plt.ylabel('Metric Value')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('data/retrained_training_curves.png', dpi=300, bbox_inches='tight')
    plt.savefig('data/retrained_training_curves.jpg', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("âœ… Training curves saved to data/retrained_training_curves.png and .jpg")

def train_model():
    """è®­ç»ƒæ¨¡å‹"""
    print("ğŸµ é‡æ–°è®­ç»ƒéŸ³ä¹åˆ†ç±»æ¨¡å‹...")
    print("=" * 50)
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    X, y, label_encoder = generate_realistic_training_data()
    
    print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y)}")
    
    # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = MusicStyleDataset(X_train_scaled, y_train)
    val_dataset = MusicStyleDataset(X_val_scaled, y_val)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # åˆå§‹åŒ–æ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = MusicStyleNet(input_size=64, num_classes=10).to(device)
    
    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)
    
    # è®­ç»ƒå†å²
    train_losses = []
    val_losses = []
    val_accuracies = []
    
    best_val_acc = 0.0
    epochs = 100
    
    print(f"å¼€å§‹è®­ç»ƒï¼Œä½¿ç”¨è®¾å¤‡: {device}")
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        for batch_features, batch_labels in train_loader:
            batch_features = batch_features.to(device)
            batch_labels = batch_labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_features)
            loss = criterion(outputs, batch_labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch_features, batch_labels in val_loader:
                batch_features = batch_features.to(device)
                batch_labels = batch_labels.to(device)
                
                outputs = model(batch_features)
                loss = criterion(outputs, batch_labels)
                val_loss += loss.item()
                
                _, predicted = torch.max(outputs.data, 1)
                total += batch_labels.size(0)
                correct += (predicted == batch_labels).sum().item()
        
        # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        val_acc = correct / total
        
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        val_accuracies.append(val_acc)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(avg_val_loss)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'data/retrained_music_model.pth')
        
        # æ‰“å°è¿›åº¦
        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}')
    
    print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curves(train_losses, val_losses, val_accuracies)
    
    # ä¿å­˜æ¨¡å‹é…ç½®
    model_config = {
        'input_size': 64,
        'num_classes': 10,
        'label_encoder': label_encoder,
        'style_descriptions': {
            'electronic': 'ç”µå­éŸ³ä¹',
            'rock': 'æ‘‡æ»šéŸ³ä¹',
            'classical': 'å¤å…¸éŸ³ä¹',
            'ambient': 'ç¯å¢ƒéŸ³ä¹',
            'pop': 'æµè¡ŒéŸ³ä¹',
            'jazz': 'çˆµå£«éŸ³ä¹',
            'hip_hop': 'å˜»å“ˆéŸ³ä¹',
            'folk': 'æ°‘è°£éŸ³ä¹',
            'chinese_traditional': 'ä¸­å›½ä¼ ç»ŸéŸ³ä¹',
            'pixel_game': 'åƒç´ æ¸¸æˆéŸ³ä¹'
        }
    }
    
    with open('data/retrained_model_config.json', 'w', encoding='utf-8') as f:
        json.dump(model_config, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜æ ‡å‡†åŒ–å™¨
    import pickle
    with open('data/retrained_scaler.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    
    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶ä¿å­˜")
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸµ é‡æ–°è®­ç»ƒéŸ³ä¹åˆ†ç±»æ¨¡å‹")
    print("=" * 60)
    
    success = train_model()
    
    if success:
        print("\nğŸ‰ æ¨¡å‹é‡æ–°è®­ç»ƒæˆåŠŸï¼")
        print("ç°åœ¨å¯ä»¥æµ‹è¯•æ–°çš„éŸ³ä¹åˆ†ç±»æ•ˆæœ")
    else:
        print("\nâŒ è®­ç»ƒå¤±è´¥")

if __name__ == "__main__":
    main() 